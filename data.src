%include('html_intro.inc')
    <meta name="keywords" content="projects">
    <meta name="description" content="NCCS TechInt group thrust area">
    <title>Technology Integration Group: Data Management</title>
%include('hdbody.inc')

%include('hdrnav.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
<div class="overview">

These projects exemplify the Technology Integration Group's
contributions to data management and analysis, including data
modeling, data capture, publication, search and discovery, analysis,
and visualization.

</div>
<div class="plinks">
<a href="#ADARA">ADARA</a> 
| <a href="#PARCAT">ParCAT</a> 
| <a href="#ESGF">Earth System Grid Federation</a>
| <a href="#ESGF-Web-FE">ESGF Web Front End</a>
| <a href="#ESGF-Search">ESGF Search</a>
| <a href="#EZ-Pub">EZ-Pub</a>
<br> <a href="#NoSQL">NoSQL Data Stores</a>
| <a href="#NIH">NIH Oncology Data</a>
| <a href="#DOI">DOIs for HPC Data</a>
</div>

%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <div class="logo-flow">
      <img class="adara" src="images/ADARA_Logo.png" alt="ADARA Logo"/>
    </div>

    <h2 class="title" id="ADARA">ADARA</h2>
    
<p>

In addition to hosting the world's fastest supercomputer, ORNL also
operates the world's brightest neutron source, the <a
href="http://neutrons.ornl.gov/" target="other">Spallation Neutron
Source</a> (SNS). Funded by the <a
href="http://science.energy.gov/bes/" target="other">US DOE Office of
Basic Energy Science</a>, this national user facility hosts hundreds
of scientists from around the world, providing a platform to enable
break-through research in materials science, sustainable energy, and
basic science. OCLF personnel have been engaged to help manage and
analyze the large data sets (ranging in size from 100's of gigabytes
to over 1 terabyte) generated by the intense pulses of neutrons.

<p>

OLCF staff and SNS data specialists collaborated to successfully
complete the Accelerating Data Acquisition, Reduction, and Analysis
(ADARA) Lab-Directed Research and Development project to improve the
production and analysis of these data sets. OLCF provided its
expertise in high-performance file systems, parallel processing,
cluster configuration and management, and data management to the
project. As a result of the ADARA project, a new data infrastructure
was created that enhances users’ ability to collect, reduce, and
analyze data as it is taken; create data files immediately after
acquisition, regardless of size; reduce a data set in seconds after
acquisition; and provide the resources for any user to do
post-acquisition reduction, analysis, visualization, and modeling
without requiring users to be on-site at the SNS facility.

<p>
ADARA is currently running on the HYSPEC beam line providing near
real-time access to result data sets (both raw event data and reduced
data) so that instrument scientists and users now obtain live
feedback from their experiments. Moving forward, ADARA will be
deployed in production across a number of beam lines at SNS as the
capabilities developed within ADARA continue to be adopted by the SNS
facility.

<p> More complete details on ADARA are available at <a
href="http://www.csm.ornl.gov/newsite/adara.html"
target="other">http://www.csm.ornl.gov/newsite/adara.html</a>.

<p>
OLCF Contributors: Feiyi Wang, Dale Stansberry, and Ross Miller

<p>
Status: On-going

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="PARCAT">ParCAT</h2>

<p>
ParCAT (parallel climate analysis tool) is a focused, easy-to-use
parallel analysis tool primarily for climate data analysis. ParCAT
parallelizes many common tasks in climate data analysis -- model
diagnostics and verification, ensemble run comparisons, and
visualization. ParCAT processes netCDF files generated by the CCSM
codes and produces netCDF files for further analysis in existing
tools. ParCAT is a lightweight "backend" command-line tool written in
C and utilizing p-NetCDF and MPI. Because it is a command line
utility, it is easily integrated in scripts or other applications
(such as EDEN or UV-CDAT). It provides parallel average map
generation, frequency hashing, differencing two datasets, and
differencing averages of two datasets. ParCAT will eventually be
released as an open source project.

<p>
Contributors: Brian Smith

<p>
Status: Completed

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="NoSQL">NoSQL Data Stores</h2>

<p>
This project explores Hadoop and key-value stores to support Big Data
analytics, and how these systems can co-exist with cloud
infrastructure. We currently support a Hadoop installation for
healthcare data analytics and are exploring similar systems for
scientific data sets and analysis.

<p>
For health-care analytics, we are building a capability named
"Cloud-get" which will provide access to large volumes of data on
virtualized servers where analysis tools can be applied to the data
directly.

<p>
Data scientists will be able to apply this capability in their domain
of interest to access and analyze large amounts of data in a flexible
way. Keeping the data in the data center will avoid the usual time
delays and security risk associated with large data transfers.
Virtualizing the analysis environment will provide a high level of
administrative control over the resources and minimize the risk
normally associated with malicious intrusions.

<p>
Contributors: Raghul Gunasekaran, James Horey (CSED), and
Seung-Hwan Lim (CSED)

<p>
Status: On-going

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')


<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="Constellation">Constellation</h2>

<p>
Constellation federates metadata from the OLCF resource fabric (stat
metadata from ~1 Billion files from Spider PFS/HPSS, millions of jobs
metadata from the scheduler, thousands of users/groups, publications
and systems), and captures them in a custom-built in-memory graph. It
builds links or associations among resources (vertices in the graph)
by correlating the metadata to infer hidden relationships (e.g.,
linking data to jobs, extracting keywords from publications to link
together relevant publications or publications, jobs and data). Graph
traversals and high-performance indexes external to the graph enable
searches.

<p>
The stat metadata index is built using Hbase and Spark queries on PFS
stat/job metadata. We also build an hierarchical index by extracting
metadata from within the datasets themselves, and create more metadata
from base metadata. Based on this graph engine, we can discover
relationships, suggest related data products/papers of interest,
identify popular datasets via pagerank, and study and create new
user-specified “tags” that tie together resources for quick retrieval
and sharing. <a href="http://users.nccs.gov/~vazhkuda/Constellation.pdf"
target="_blank">[ConstellationGraph:BigData16]</a>

<p>
The first workflow to be supported in Constellation is the acquisition
of Digital Object Identifiers for scientific datasets (see "DOIs for
HPC Data" below).

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="DOI">DOIs for HPC Data</h2>

<p>
Recent directives from federal agencies outline a new desire and
requirement to provide access to scientific data arising from
taxpayer-funded research. The provision of this data will require new
policies and procedures including a much-improved mechanism for dataset
identification and tracking.

<p>
To this end, we are exploring the viability of digital object identifiers
(DOI) as a means to track data products emanating from scientific
simulations. DOI is a mechanism that can be used to help track,
identify, and share the data sets that are produced by researchers
globally. 

<p> 
The ability to facilitate data-related services via DOIs has new uses for
both the HPC center and the end-user. The center could utilize DOIs in their
interactions with funding agencies by providing improved accounting and
visibility of our user facility's data production. The center can also directly
benefit from new "data strategies" such as data warehousing and other beneficial
schemes that result from the improved planning information associated with DOIs
and Data Management Planning. From a user standpoint, DOIs help facilitate data
sharing, enable publication credit, enable data preservation beyond the lifetime
of the project at the center, facilitate lineage tracking, and facilitate the
use of intermediate data products.

<p>
We are working on creating a workflow so that users can obtain DOIs for
their data products of interest. We are working with OSTI in trying to
create this infrastructure.

<p>
Contributors: Sudharshan Vazhkudai and Doug Fuller

<p>
Status: Ongoing

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

%include('html_close.inc')
