%include('html_intro.inc')
    <meta name="keywords" content="projects">
    <meta name="description" content="NCCS TechInt group thrust area">
    <title>Technology Integration Group: Data Management</title>
%include('hdbody.inc')

%include('hdrnav.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
<div class="overview">

Data management and analysis is the focus of this area. The projects
below exemplify the Technology Integration Group's contributions to
data management and analysis, from data modeling or capture through
publication, search and discovery, and retrieval to analysis and
visualization.

</div>
<div class="plinks">
<a href="#ADARA">ADARA</a> 
| <a href="#PARCAT">ParCAT</a> 
| <a href="#ESGF">Earth System Grid Federation</a>
| <a href="#ESGF-Web-FE">ESGF Web Front End</a>
| <a href="#ESGF-Search">ESGF Search</a>
| <a href="#EZ-Pub">EZ-Pub</a>
<br><a href="#NIH">NIH Oncology Data</a>
</div>

%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="ADARA">ADARA</h2>
<p>
In addition to hosting the world's fastest supercomputer, ORNL also
operates the world's brightest neutron source, the Spallation Neutron
Source (SNS). Funded by the US DOE Office of Basic Energy Science,
this national user facility hosts hundreds of scientists from around
the world, providing a platform to enable the break-through research
in materials science, sustainable energy, and basic science. OCLF
personnel have been engaged to help manage and analyze the large data
sets (ranging in size from 100's of GB to over 1 TB) generated by the
intense pulses of neutrons.

<p>
OLCF staff and SNS data specialists collaborated to successfully
complete the Accelerating Data Acquisition, Reduction, and Analysis
(ADARA) Lab-Directed Research and Development project to improve the
production and analysis of these data sets. OLCF provided its
expertise in high-performance file systems, parallel processing,
cluster configuration and management, and data management to the
project. As a result of the ADARA project, a new data infrastructure
was created that enhances users’ ability to collect, reduce, and
analyze data as it is taken; create data files immediately after
acquisition, regardless of size; reduce a data set in seconds after
acquisition; and provide the resources for any user to do
post-acquisition reduction, analysis, visualization, and modeling
without requiring users to be on-site at the SNS facility.

<p>
ADARA is currently running on the HYSPEC beam line providing near
real-time access to result data sets (both raw event data and reduced
data) so that instrument scientists and users now obtain live
feedback from their experiments. Moving forward, ADARA will be
deployed in production across a number of beam lines at SNS as the
capabilities developed within ADARA continue to be adopted by the SNS
facility.

<p>
Contributors: Dave Dillow, Feiyi Wang, Dale Stansberry, and Ross Miller

<p>
Status: On-going

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="PARCAT">ParCAT</h2>

<p>
ParCAT (parallel climate analysis tool) is a focused, easy-to-use
parallel analysis tool primarily for climate data analysis. ParCAT
parallelizes many common tasks in climate data analysis -- model
diagnostics and verification, ensemble run comparisons, and
visualization. ParCAT processes netCDF files generated by the CCSM
codes and produces netCDF files for further analysis in existing
tools. ParCAT is a lightweight "backend" command-line tool written in
C and utilizing p-NetCDF and MPI. Because it is a command line
utility, it is easily integrated in scripts or other applications
(such as EDEN or UV-CDAT). It provides parallel average map
generation, frequency hashing, differencing two datasets, and
differencing averages of two datasets. ParCAT will eventually be
released as an open source project.

<p>
Contributors: Brian Smith

<p>
Status: Completed

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="ESGF">Earth System Grid Federation</h2>

<p> 
The study of climate change, and the evaluation of its impact on
the Earth ecosystem and human society, is one of the most important
scientific challenges of our time. Because of the extreme diversity
and complexity of the physical processes that govern the Earth
climate, this research involves the use of sophisticated model
simulations generating unprecedented amount of output data, as well as
the collection of observational data from multiple sources (remote
sensing, in-situ, vertical profiles etc.) on a global scale. These
data sets are managed and stored at multiple geographic locations all
across the globe, yet they need to be discovered, accessed and
analyzed as if they were stored in a single centralized archive.

<!--
<p>
For over a decade, the objective has been to establish the Earth
System Grid Federation (ESGF) as a leader and visionary architect for
all aspects of climate data discovery and knowledge integration. From
its formative years (now known as the Earth System Grid (ESG)), ESGF
has demonstrated a viable path forward into the future and attracted
key national and international collaborators and potential sponsors.
To date, global funding sponsors have started to achieve a critical
mass to undertake fundamental data-intensive and data-driven science
problems. In the climate application arena, projects are consolidating
on the ESGF Peer-to-Peer (P2P) infrastructure. This realization places
ESGF at the forefront and allows it to play a stewardship role in the
transformation in climate science that will lead to integration of
data driven infrastructure and analysis with science research and
discoveries. It enables the world to better organize and integrate all
climate knowledge via a cooperative federation. With that said, the
ESGF P2P software has distinguished itself from other collaborative
knowledge systems in the climate community, as evidenced by its
widespread adoption, federation capabilities, and broad developer
base. It is the leading source for current climate data holdings,
including the most important and largest data sets in the
global-climate community.
-->

<p> 
Through ESGF, users access, analyze, and visualize data using a
globally federated collection of networks, computers, and software.
The enterprise system supports national and international climate
model and observation intercomparison activities as well as high
profile U.S. DOE, NOAA, NASA, and NSF projects.

<p>

ORNL has played a critical role in several sub projects that have been
crucial to the success of the ESGF project, each described below.
First, ORNL leads the design and production of the ESGF Web portal
interface module. This browser-based application provides an intuitve
interface for users to access all of the data management services
offerred by ESGF. Second, ORNL has contributed in developing the ESGF
search and discovery project. This tool allows users to find data
across the federated contents of ESGF in a quick and easy manner.
Third, ORNL has led in the development of a new development tool
called EZ-Pub. Ez-Pub enables scientists to use a simplified process
to publish data into the ESGF system. Finally, ORNL is currently
producing the ESG-SRM project, which provides a web API for extracting
files from deep storage archives such as HPSS.

<p>
Details about other ESGF related projects can be seen on the <a
href="http://esgf.org" target="other">ESGF main page</a>.

<p>Contributors: John Harney, Feiyi Wang, Tom Barron, and Ross Miller

<p>Status: On-going

<p>Selected Publications
<p>
<ul>

  <li>Feiyi Wang, John Harney, Galen Shipman, Dean Williams, Luca
  Cinquini, "Building a Large-Scale Climate Data System for HPC
  Environments", published in IEEE 7th International Conference on
  Next Generation Web Service Practices, Oct 19-21. Salamanca,
  Spain.</li>

  <li>Luca Cinquini, Daniel Crichton, Chris Mattmann, John Harney,
  Galen Shipman, Feiyi Wang, Rachana Ananthakrishnan, Neill Miller,
  Sebastian Denvil, Mark Morgan, Zed Pobre, Gavin M. Bell, Charles
  Doutriaux, Robert Drach, Dean Williams, Phillip Kershaw, Stephen
  Pascoe, Estanislao Gonzalez, Sandro Fiore, and Roland Schweitzer.
  The Earth System Grid Federation: An Open Infrastructure for Access
  to Distributed Geospatial Data. To appear in Proceedngs of the 8th
  IEEE International Conference on eScience 2012, Chicago, IL, Oct
  8-12, 2012.</li>

  <li>Luca Cinquini, Daniel Crichton, Chris Mattmann, John Harney,
  Galen Shipman, Feiyi Wang, Rachana Ananthakrishnan, Neill Miller,
  Sebastian Denvil, Mark Morgan, Zed Pobre, Gavin M. Bell, Charles
  Doutriaux, Robert Drach, Dean Williams, Phillip Kershaw, Stephen
  Pascoe, Estanislao Gonzalez, Sandro Fiore, and Roland Schweitzer.
  The Earth System Grid Federation: An Open Infrastructure for Access
  to Distributed Geospatial Data. To appear in special issue of FGCS
  2013 (Future Generation Computing Systems).</li>

  <li>Feiyi Wang, John Harney, Tom Barron, Galen Shipman, Dean
  Williams, Luca Cinquini, "Design and Implementation of a Scalable
  Climate Data System in Support of HPC Environments". <em>In revision
  stage</em>.</li>

</ul>
  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 id="ESGF-Web-FE">ESGF Web Front End </h2>

<p>
The ESGF Web Front End project provides an intuitive and easy-to-use
web portal, satisfying a key goal of the ESGF development effort. ESGF
has adopted a modern design inspired by emerging Web 2.0 technologies.
The portal makes heavy use of the state-of-the-art JQuery JavaScript
libraries and Model-View-Controller (MVC) design patterns as
implemented by the Spring Framework for Java. The user interface (UI)
also conforms to the overall ESGF theme of decoupled software
components that accommodate a more dynamic user experience. Services
of other components (e.g. search and node manager components) may be
utilized via Ajax-style HTTP calls. The web UI materializes a variety
of ESGF supported services through the browser: (1) administrative and
user account management interfaces, (2) collection and file level
search and discovery, (3) the dashboard service, (4) the Live Access
Server (LAS) visualization tool and (5) the Common Information Model
(CIM) viewer.

<!--
<p>
The user account and administrative component delivers a clear and
concise means for users and administrators to manage personal
information and security aspects of the portal. Users may view and
edit personal attributes and apply for group memberships that grant
access to secure data. Administrators are provided with tools that
interface to the security infrastructure services that allow group
access to specific data holdings.  Search and discovery mechanisms are presented to the user via the
search component of web portal. The search component is presented as a
single, composite page, allowing users to avoid clumsy
“back-and-forth” static page navigation through the portal.
Initially, the user may search the dataset holdings at the collection
level. Here, the user has a plethora of search options. The sidebar
widget located on the left side of the page presents a configurable
list of facets, which allow users to constrain their search results
through categorization. Search constraints can also be added via the
free text search bar. Furthermore, users may take advantage of the
temporal and geospatial (using Google Maps) search overlay
widgets by searching for data measured or simulated at specific times
or geographical locations, respectively. The state of the search
(i.e. the current constraints utilized by the user) is shown
prominently in the upper left corner of the page and may be flexibly
changed at any time. Important information about the datasets may
also be viewed through the search component of the portal. Each
dataset contains a comprehensive metadata summary outlining
descriptive information about the dataset. The CIM viewer, which
provides richer and more detailed metadata about climate models, may
also be invoked. Additional information associated with a dataset,
such as a technical report, may also be accessed. Finally, the Live
Access Server (LAS) browser visualization tool may also be invoked to
visualize details of the datasets. For file access, the search
component uses a “shopping cart” metaphor commonly used in e-Commerce
sites. The user needs only to select a collection level dataset (via
the “Add to Cart” link) and navigate to the “Data Cart” tab.  The
collections previously chosen are displayed in this tab and may be
quickly expanded to their file level data holdings. These files may
then be downloaded by means of a number of different methods that are
discussed in the next section.
-->

<p>
The ORNL ESGF Portal site is located at <a
href="http://esg.ccs.ornl.gov" target="other">http://esg.ccs.ornl.gov</a>

<p>
Contributors: John Harney

<p>
Status: On-going
  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 id="ESGF-Search">ESGF Search</h2>

<p>
The ESGF search module, deployed as part of an ESGF Index Node,
makes the data holdings of one or more ESGF Data
Nodes discoverable and accessible by clients. Its two main features
are the ability to execute distributed searches across the federation,
and the support for a well defined REST API that insulates clients
from the specific query syntax of the back-end metadata repository.
Although the software is architected to allow for pluggable back-ends,
currently it uses <a href="http://lucene.apache.org/solr/"
target="other">Apache Solr</a> as the underlying search engine. Solr
is a very popular search engine, used in many commercial web sites,
that features high performance text and faceted searching, geospatial
and temporal querying, and partition of searchable metadata across
multiple local indexes (“cores”) and distributed servers (“shards”).

<p>
The search module makes two high-level web services
available to clients:

<ul>

 <li>

 The Indexing Service is used to parse the metadata content
 available at some repository (located by its URL), and ingest it in
 the back-end metadata storage. So far ESGF has focused mostly on
 parsing metadata from <a
 href="http://www.unidata.ucar.edu/projects/THREDDS/" target="other">THREDDS</a>
 catalogs, although support for
 other repositories such as <a href="http://gcmd.nasa.gov"
 target="other">GCMD</a> archives is forthcoming. Indexing
 operations are subject to authorization according to the local
 policies established at the Index Node: the client must present a
 valid SSL certificate belonging to a user that has been granted
 publishing rights for that specific data collection. Because of
 historical reasons, the Indexing Service is currently exposed through
 the Hessian protocol.

 <li>The Search Service is used to query the index metadata content
 to retrieve matching results that include descriptive information, as
 well as all the available data access endpoints (HTTP, GridFTP,
 OpenDAP, LAS etc…). The search service is invoked by clients through
 a REST API that includes the ability to execute free-text queries
 (for example “climate”, “surface temperature”, etc.), as well as
 searching for specific values of one or more search categories or
 “facets” (for example, “model=CESM and time frequency=monthly”).
 Among other things, the API allows clients to search for either the
 latest or a specific version of the data, for replicas, and to target
 only the local index or any number of remote indexes. Response
 documents can be returned in either XML or JSON format.
</ul>

<p>
Internally, in order to optimize performance, the search module
partitions its metadata space into different object types, that are
stored and queried separately. Datasets contain high-level information
about a data collection (such as the model that generated it or the
instrument that collected it, or its temporal and geo-spatial
coverage), and they are typically returned as results matching a
query, In other words, a Dataset is the unit of discovery in the
system. Files and Aggregations represent the content of a Dataset, and
contain the URL endpoints of all services that can be used by clients
to access the data. Typically, a single Dataset contains many Files
and/or Aggregations, so searching on Datasets is much faster.

<p>
Within Solr, each object type is stored in a dedicated “core” or
index, which is separately updated and queried. Additionally, the Solr
setup at each Node is such that metadata is indexed into a master Solr
instance, which is only accessible locally, and then replicated at
one-minute intervals to a slave Solr instance, which is accessible for
read-only operations by outside clients.

<p>
Contributors: John Harney

<p>
Status: On-going
  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 id="EZ-Pub">EZ-Pub</h2>

<p>
The Search and Front-End components help users gain access to data.
For data to be available for discovery and download, it must first be
published into the system. EZ-Pub provides a simple, stream-lined
method for climate scientists to share their data with colleagues
through the ESG Federation.

<p>
EZ-Pub conducts a dialog with the user to collect metadata describing
the files to be published.

<p>
After collecting the user-provided metadata, EZ-Pub can work with
multiple mechanisms to stage the metadata and data to the data node
where it will be housed. Staging options include GridFTP Lite and
Globus Online.

<p>
Once data files have been staged, they are scanned to collect internal
metadata like file sizes, modification times, variable names, units,
etc., and that information is formatted into xml files suitable for
inclusion in a THREDDS catalog. These xml files are added to the
THREDDS catalog and the THREDDS server is re-initialized so that it
will be aware of the new files.

<p>
Finally, Solr is invoked to harvest the new metadata from THREDDS so
it will appear in the search and discovery interface.

<p>
Contributors: Tom Barron

<p>
Status: On-going
<!--
    <h3>ESGF SRM</h3>
-->

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="NoSQL">NoSQL Data Stores</h2>

<p>
This project explores Hadoop and key-value stores to support Big Data
analytics, and how these systems can co-exist with cloud
infrastructure. We currently support a Hadoop installation for
healthcare data analytics and are exploring similar systems for
scientific data sets and analysis.

<p>
For health-care analytics, we are building a capability named
"Cloud-get" which will provide access to large volumes of data on
virtualized servers where analysis tools can be applied to the data
directly.

<p>
Data scientists will be able to apply this capability in their domain
of interest to access and analyze large amounts of data in a flexible
way. Keeping the data in the data center will avoid the usual time
delays and security risk associated with large data transfers.
Virtualizing the analysis environment will provide a high level of
administrative control over the resources and minimize the risk
normally associated with malicious intrusions.

<p>
Contributors: Raghul Gunasekaran, James Horey (CSED), and
Seung-Hwan Lim (CSED)

<p>
Status: On-going

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')


<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 class="title" id="NIH">A Portal for Oncology Data</h2>

<p>
The purpose of this proposal is to define, build and host a pilot
informatics infrastructure data center to support the twelve NIH
Physical Sciences-Oncology Centers (PS-OC). The proposal will cover
all aspects of the pilot informatics system, including documentation,
front end web portal, software deployment, back-end data access and
storage, data security, data visualization and user support. The
deliverable is a prototype system and feasibility study for a
long-term PS-OC Data Coordinating Center (PS-OC DCC).

<p>
Contributors: Sudharshan Vazhkudai, Ali Passian (MSED), and Chao Wang

<p>
Status: Ongoing

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

%include('html_close.inc')
