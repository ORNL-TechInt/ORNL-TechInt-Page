%include('html_intro.inc')
    <meta name="keywords" content="projects">
    <meta name="description" content="NCCS TechInt group thrust area">
    <title>Technology Integration Group: Storage</title>
%include('hdbody.inc')

%include('hdrnav.inc')

<!-- For each project, we want to include information about 1) who
     works on it, 2) what its status is (completed, ongoing, in
     maintenance, etc.), 3) links to related software or papers on the
     software/publications pages, and 4) funding agency -->

%include('content_intro.inc')
    <h2 id="Spider" class="title">Spider</h2>

<p>
Spider, a Lustre-based center-wide ﬁle system, delivers up to 240 GB/s
of ﬁle system level throughput and provides 10.7 PB of formatted disk
capacity to NCCS users. Since it is connected to all major NCCS
computational resources, it provides a means of sharing data among
platforms, reducing the need for long transfers of large files between
machines.

<p>
Through careful analysis, Spider was engineered to be fault tolerant,
so that if one platform using Spider fails, the impact on other platforms using the
storage system is minimized.

<p>
Spider's architecture also allows for the addition of storage capacity
with minimal reconfiguration so that the storage pool can 
scale as needed to support center requirements.
%include('content_close.inc')

%include('content_intro.inc')
    <h2 id="HPSS" class="title">HPSS</h2>

<p>
HPSS is the result of over a decade of collaboration among five
Department of Energy laboratories and IBM, with significant
contributions by universities and other laboratories worldwide.

<p>
HPSS can manage petabytes of data on disk and robotic tape libraries,
providing highly flexible and scalable hierarchical storage management
that keeps recently used data on disk and less recently used data on
tape. Through the use of cluster, LAN and/or SAN technology, HPSS
aggregate the capacity and performance of many computers, disks, and
tape drives into a single virtual file system of exceptional size and
versatility. This approach enables HPSS to easily meet otherwise
unachievable demands of total storage capacity, file sizes, data
rates, and number of objects stored.

<p>
TechInt members developed and maintain components of HPSS responsible
for the administrative interfaces, low level data management, and
logging.

<p>
Contributors: Deryl Steinert, Vicky White, and Tom Barron
%include('content_close.inc')

%include('content_intro.inc')
    <h2 id="Spider2" class="title">Spider2</h2>
%include('content_close.inc')


%include('content_intro.inc')
    <h2 id="LUT" class="title">Luster User Tools (LUT)</h2>
%include('content_close.inc')


%include('content_intro.inc')
    <h2 id="Ceph" class="title">Ceph</h2>
%include('content_close.inc')


%include('content_intro.inc')
    <h2 id="Benchmarks" class="title">Benchmarks</h2>
%include('content_close.inc')


%include('content_intro.inc')
    <h2 id="LustreEval" class="title">Lustre Evaluation</h2>
%include('content_close.inc')


%include('content_intro.inc')
    <h2 id="IOCoord" class="title">I/O Coordination</h2>

<p>
Large-scale systems are heavily-shared resource environments where a
mix of applications run concurrently and compete for network and
storage resources. It is essential to characterize the runtime
behavior of these applications in order to provision system resources
and understand the impact of resource contention on an application’s
performance.


<p>
Traditionally, applications are characterized and benchmarked using
ﬁne-grained performance tools on a quiescent machine, however, the
runtime behavior of the applications tend to be impacted differently by
the availability of shared resources and also varies based on the
needs of the specific scientific user.

<p>
The objective of this project is to characterize the I/O usage
patterns of user-applications from the perspective of backend storage
servers while minimizing overhead and interference with the
application's I/O activities, and build an I/O-aware decision making
capability, to guide decisions about issues such as scheduling.

<p>
At OLCF, the spider storage system is available as four partitions and
the users are allowed to use all or any of them during runtime. The
immediate goal of the project is to enable scientific users to choose
the best file system partition based on current loads and known I/O
behavior of their application.

<p>
Contributors: Raghul, Sudharshan, Yang Liu (NCSU), Xiaosong Ma (NCSU)
 
%include('content_close.inc')


%include('content_intro.inc')
    <h2 id="SOS" class="title">Scalable Object Storage</h2>
%include('content_close.inc')


%include('content_intro.inc')
    <h2 id="NOAA" class="title">NOAA</h2>
%include('content_close.inc')
%include('html_close.inc')
