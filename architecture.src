%include('html_intro.inc')
    <meta name="keywords" content="projects">
    <meta name="description" content="NCCS TechInt group thrust area">
    <title>Technology Integration Group: System Architecture and Resilience</title>
%include('hdbody.inc')

%include('hdrnav.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
<div class="overview">

These projects relate to innovations in system architecture that aim
to improve performance, reduce variability, and enhance
reliability/resilience. They may also involve evaluation of new
processor technologies as applicable to the OLCF roadmap.

</div>
<div class="plinks">
<a href="#TBSchedule">Top and Bottom Scheduling</a>
<br><a href="#Placement">Impacts of Job Placement and Routing Topology on Aggregate I/O Throughput</a>
<br><a href="#Many-Core">Exploiting Many-Core Architectures for End-to-End Science</a>
<br><a href="#OSNoise">Reducing Application Runtime Variability on Jaguar XT5</a>
<br><a href="#Process-Placement">Improving Application Performance by Avoiding Floating-Point Unit Contention on Titan</a>
</div>

%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')

    <h2 id="TBSchedule">Top and Bottom Scheduling</h2>

    <p>
    To minimize the runtime variability of jobs and reduce node
    allocation fragmentation, we developed a dual-ended scheduling
    policy for the Moab scheduler on Titan as opposed to the first-fit
    policy. Our algorithm schedules large jobs top down and small jobs
    bottom up, thereby minimizing the fragmentation for large,
    long-running jobs that is caused by small, short-lived jobs.
    Further, we also modified the scheduling of nodes to a job to
    prioritize the z dimension (nodes within a rack), followed by the
    x dimension (nodes in the racks that are in the same row offer
    better communication bandwidth) and then the y dimension (columns).

    <p>
    Thousands of jobs are benefitting from this technique. This
    project received a Significant Event Award, a prestigious lab-wide
    recognition.

    <p>Contributors: Chris Zimmer, Scott Atchley, Sudharshan Vazhkudai

    <p>Status: Complete

    <p>Selected publications:
    <ul>
    <li>
    Christopher Zimmer, Saurabh Gupta, Scott Atchley, Sudharshan S.
    Vazhkudai, and Carl Albing, "A Multi-faceted Approach to Job
    Placement for Improved Performance on Extreme-Scale Systems,"
    <i>Proceedings of Supercomputing 2016 (SC16): 29th Int'l
    Conference on High Performance Computing, Networking, Storage and
    Analysis</i>, Salt Lake City, UT, November 2016.
    </ul>

%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')

    <h2 id="Placement">Impacts of Job Placement and Routing Topology
    on Aggregate I/O Throughput</h2>

    <p>High-bandwidth file-systems are critical to todayâ€™s
    super-computing applications. To achieve the level of performance
    necessary for leadership class of applications, the underlying
    network must facilitate high aggregate-bandwidth demands.
    Unfortunately, in such a large-scale network, congestion at
    routers leads to limited overall I/O performance and high
    variability.

    <p>In this effort, our goal is to identify and understand
    bottlenecks in the interconnection-network pertaining to the file
    system I/O traffic. This work will involve analyzing impact of job
    placement, router placement on performance, and studying how these
    configurations play a role in reducing congestion in the
    interconnection-network. During the course of this investigation
    we seek to:

    <ol class="numbered">

    <li class="numbered">Understand the dynamic behavior of our current system via
    careful experimentation and investigation, and identify where
    bottlenecks occur,</li>

    <li>Develop simulated models of the network for an in-depth study
    of network dynamics in presence of real-life workload scenarios,</li>

    <li>Identify heuristics or layouts that improve upon the existing
    topology, and

    <li>Come up with optimal I/O router layouts for current and
    several other network topologies for future systems.

    </ol>

    <p>Contributors: Saurabh Gupta, Chris Zimmer, and Sudharshan Vazhkudai

    <p>Status: On-going

%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 id="Many-Core">Exploiting Many-Core Architectures for End-to-End Science</h2>

<p>With the leveling off of processor clockspeeds, chip manufacturers have
increased the number of cores to consume the additional transitors promised by
Moore's Law. As we move towards exascale systems, we may see higher core counts
per node including more cores per socket, more sockets, and add-in boards such
as GP-GPUs and many-core coprocessors connected via PCI Express. As users
initially port their applications to Titan's GP-GPUs, they may not fully utilize
the CPUs leaving them available for functional partitioning.</p>

<p>This effort will explore providing runtime services to applications using a
small subset of cores on a many-core systems. These services may include I/O
buffering, which would allow the application to resume computation more quickly,
or various forms of fine-grain analysis or transformation.</p>

    <p>Contributors: Scott Atchley, Saurabh Gupta, Ross Miller, and
    Sudharshan Vazhkudai

    <p>Status: On-going

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')

   <h2 id="OSNoise">Reducing Application Runtime Variability on
   Jaguar XT5</h2>

<p>

Operating system (OS) noise is defined as interference generated by
the OS that prevents the compute core from performing useful work.
Compute node kernel daemons, network interfaces, and other OS related
services are major sources of such interference. This interference on
individual compute cores can vary in duration and frequency and can
cause de-synchronization (jitter) in collective communication tasks
and thus results in variable (degraded) overall parallel application
performance. This behavior is more observable in large-scale
applications using certain types of collective communication
primitives, such as MPI_Allreduce. Our efforts focused towards
reducing the overall effect of OS noise on our large-scale parallel
applications. We performed tests on the quad-core Jaguar, the Cray XT5
at the Oak Ridge National Laboratory Leadership Computing Facility
(OLCF). At the time of these tests, Jaguar was a 1.4 petaflop/second
supercomputer with 144,000 compute cores and 8 cores per node. The
technique we used was to aggregate and merge all OS noise sources onto
a single compute core for each node. The scientific application was
then run on the remaining seven cores in each node. Our results showed
that we were able to improve the MPI_Allreduce performance by two
orders of magnitude and to boost the Parallel Ocean Program (POP)
performance over 30% using this technique.


<p>
Contributors: Dave Dillow, Don Maxwell, Ross Miller, Sarp Oral, Galen
Shipman, and Feiyi Wang, Jeff Becklehimer (Cray), and Jeff Larkin (Cray)

<p>
Status: Completed

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 id="Process-Placement">Improving Application Performance by Avoiding Floating-Point Unit Contention on Titan</h2>

<p>Titan's compute nodes include an AMD Opteron&#8482; 6274 (Interlagos) CPU and a Nvidia Kepler&#8482; GPGPU. Applications may use the CPU, GPGPU, or both. The <a href="https://www.olcf.ornl.gov/kb_articles/xk7-cpu-description/" target="other">Interlagos CPU</a> has eight <i>bulldozer</i> modules. Each bulldozer has two integer units and one floating-point unit. Applications, which are floating-point intensive and which use the CPU for some or all computation, may see improved performance (i.e. reduced walltime) if they place one process per bulldozer. Typical application speedup is 1.5-2.0, with an average of 1.7.</p>

<p>This effort provides feedback to the user when their job submission tries to reduce floating-point unit contention, but the user fails to set the requisite flags to obtain the improved performance. We are also exploring the ability to provide alternative job submission options, which will set the appropriate flags for the user based on the type of job.</p>

    <p>Contributors: Scott Atchley and Sudharshan Vazhkudai

    <p>Status: On-going

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

%include('html_close.inc')

