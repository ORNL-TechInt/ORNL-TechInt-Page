%include('html_intro.inc')
    <meta name="keywords" content="projects">
    <meta name="description" content="NCCS TechInt group thrust area">
    <title>Technology Integration Group: System Architecture and Resilience</title>
%include('hdbody.inc')

%include('hdrnav.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
<div class="overview">

These projects relate to innovations in system architecture that aim
to improve performance, reduce variability, and enhance
reliability/resilience. They may also involve evaluation of new
processor technologies as applicable to the OLCF roadmap.

</div>
<div class="plinks">
<a href="#Many-Core">Functional Partitioning Runtime for Many-Core Nodes</a>
<br><a href="#TBSchedule">Top and Bottom Scheduling</a>
<br><a href="#SCFailure">Supercomputer Failure Analysis</a>
<br><a href="#Congestion">Interconnect Congestion Analysis</a>
<br><a href="#Hetero">Heterogeneous Processor Architectures and End-to-end Computing</a>
</div>

%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')
    <h2 id="Many-Core">Functional Partitioning Runtime for Many-Core Nodes</h2>

<p>With the leveling off of processor clockspeeds, chip manufacturers have
increased the number of cores to consume the additional transitors promised by
Moore's Law. As we move towards exascale systems, we may see higher core counts
per node including more cores per socket, more sockets, and add-in boards such
as GP-GPUs and many-core coprocessors connected via PCI Express. As users
initially port their applications to Titan's GP-GPUs, they may not fully utilize
the CPUs leaving them available for functional partitioning.</p>

<p>This effort will explore providing runtime services to applications
using a small subset of cores on a many-core systems by developing a
Functional Partitioning (FP) runtime environment. This environment
will partition a many-core node such that an end-to-end application
(simulation + data analysis tasks) can be scheduled in-situ, on the
same node, alongside the application’s simulation job for better
end-to-end performance.

<p> Services provided may include I/O buffering, which would allow the
application to resume computation more quickly, or various forms of
fine-grain analysis or transformation.</p>

    <p>Contributors: Scott Atchley, Saurabh Gupta, Ross Miller, and
    Sudharshan Vazhkudai

    <p>Status: On-going

<P>Selected publications:
<ul><li>
M. Li, S. S. Vazhkudai, A.R. Butt, F. Meng, X. Ma, Y. Kim, C.
Engelmann, G. Shipman, "Functional Partitioning to Optimize End-to-End
Performance on Many-core Architectures", <i>Proceedings of Supercomputing
2010 (SC10): 23rd IEEE/ACM Int'l Conference on High Performance
Computing, Networking, Storage and Analysis</i>, New Orleans, Louisiana,
November 2010.
<a href="http://users.nccs.gov/~vazhkuda/FP.pdf">pdf</a>
</ul>

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')

    <h2 id="TBSchedule">Top and Bottom Scheduling</h2>

    <p>
    To minimize the runtime variability of jobs and reduce node
    allocation fragmentation, we developed a dual-ended scheduling
    policy for the Moab scheduler on Titan as opposed to the first-fit
    policy. Our algorithm schedules large jobs top down and small jobs
    bottom up, thereby minimizing the fragmentation for large,
    long-running jobs that is caused by small, short-lived jobs.
    Further, we also modified the scheduling of nodes to a job to
    prioritize the z dimension (nodes within a rack), followed by the
    x dimension (nodes in the racks that are in the same row offer
    better communication bandwidth) and then the y dimension (columns).

    <p>
    Thousands of jobs are benefitting from this technique. This
    project received a Significant Event Award, a prestigious lab-wide
    recognition.

    <p>Contributors: Chris Zimmer, Scott Atchley, Sudharshan Vazhkudai

    <p>Status: Complete

    <p>Selected publications:
    <ul>
    <li>
    Christopher Zimmer, Saurabh Gupta, Scott Atchley, Sudharshan S.
    Vazhkudai, and Carl Albing, "A Multi-faceted Approach to Job
    Placement for Improved Performance on Extreme-Scale Systems,"
    <i>Proceedings of Supercomputing 2016 (SC16): 29th Int'l
    Conference on High Performance Computing, Networking, Storage and
    Analysis</i>, Salt Lake City, UT, November 2016.
    <a href="http://users.nccs.gov/~vazhkuda/Titan-Scheduling.pdf">pdf</a>
    </ul>

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')

    <h2 id="SCFailure">Supercomputer Failure Analysis</h2>

    <p>
    This project analyzes temporal failure characteristics of Titan’s
    299,008 CPUs and 18,688 GPUs to understand trends in machine
    failure, MTBF, single bit errors, double bit errors, off the bus
    errors and temperature correlation to failure
    [TitanGPUReliability:HPCA15]. This study was the first of its kind
    for a large-scale GPU deployment. Based on the insights, devised
    checkpointing advisory tools [LazyChkpt:DSN14] that are saving
    millions of core hours for production jobs, devised an
    energy-based scheduling algorithm to schedule large node-count GPU
    jobs that stress the GPUs more to be scheduled at the bottom of
    the rack where it is much cooler than the top of the rack, or
    cordoned off nodes with frequent failures

    <p>TechInt contributors: Devesh Tiwari, Sudharshan Vazhkudai

    <p>Status: Complete

    <p>Selected publications:
    <ul>
    <li>
    Devesh Tiwari, Saurabh Gupta, Jim Rogers, Don Maxwell, Paolo Rech,
    Sudharshan Vazhkudai, Daniel Oliveira, Dave Londo, Nathan
    Debardeleben, Philippe Navaux, Luigi Carro, Arthur Buddy Bland,
    "Understanding GPU Errors on Large-scale HPC Systems and the
    Implications for System Design and Operation", <i>21st IEEE Symp.
    on High Performance Computer Architecture (HPCA)</i>, San
    Francisco, California, February 2015.
    <a href="http://users.nccs.gov/~vazhkuda/hpca.pdf">pdf</a>
    <li>
    Devesh Tiwari, Saurabh Gupta, Sudharshan S. Vazhkudai, "Lazy
    Checkpointing: Exploiting Temporal Locality in Failures to
    Mitigate Checkpointing Overheads on Extreme-Scale Systems",
    <i>Proceedings of the 44th Annual IEEE/IFIP Int'l Conference on
    Dependable Systems and Networks (DSN 2014)</i>, Atlanta, Georgia,
    June 2014.
    <a href="http://users.nccs.gov/~vazhkuda/DSN2014.pdf">pdf</a>
    </ul>

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')

    <h2 id="Congestion">Interconnect Congestion Analysis</h2>

    <p>High-bandwidth file-systems are critical to today’s
    super-computing applications. To achieve the level of performance
    necessary for leadership class of applications, the underlying
    network must facilitate high aggregate-bandwidth demands.
    Unfortunately, in such a large-scale network, congestion at
    routers leads to limited overall I/O performance and high
    variability.

    <p>In this effort, our goal was to identify and understand
    bottlenecks in the interconnection-network pertaining to the file
    system I/O traffic. This work involved analyzing the impact of job
    placement, router placement on performance, and studying how these
    configurations play a role in reducing congestion in the
    interconnection-network. During the course of this investigation
    we sought to:

    <ol class="numbered">

    <li class="numbered">Understand the dynamic behavior of the system via
    careful experimentation and investigation, and identify where
    bottlenecks occur,</li>

    <li>Develop simulated models of the network for an in-depth study
    of network dynamics in presence of real-life workload scenarios,</li>

    <li>Identify heuristics or layouts that improve upon the existing
    topology, and

    <li>Come up with optimal I/O router layouts for current and
    several other network topologies for future systems.
    </ol>

    <p>
    As a result of this research, we developed and deployed a
    lightweight, scalable mechanism to monitor the router traffic on
    Titan’s interconnect fabric (9600 routers). The tool is used to
    analyze interference among jobs.

    <p>Contributors: Saurabh Gupta, Chris Zimmer, and Sudharshan Vazhkudai

    <p>Status: On-going

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
%include('content_intro.inc')

   <h2 id="Hetero">Heterogeneous Processor Architectures and
   End-to-end Computing</h2>

<p>
(i) Studied how end-to-end workflows (simulations + data analysis)
perform on heterogeneous processor architectures such as big
processors (AMD Opterons, XEONs), Coprocessors (GPUs, MIC) and little
processors (ARM, ATOM), considering several competing dimensions such
as performance, on-chip scalability, energyefficiency, and error
resiliency to provide insights on the design of future heterogeneous
systems for end-to-end workflows.

<p>
(ii) Procured an ARM-based cluster,
and making it available to a community of users to study the
architecture’s viability for exascale.

<p>
Contributors: ?

<p>
Status: Completed

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

<!-- ========================================================== -->
<!--
%include('content_intro.inc')

   <h2 id="OSNoise">Reducing Application Runtime Variability on
   Jaguar XT5</h2>

<p>

Operating system (OS) noise is defined as interference generated by
the OS that prevents the compute core from performing useful work.
Compute node kernel daemons, network interfaces, and other OS related
services are major sources of such interference. This interference on
individual compute cores can vary in duration and frequency and can
cause de-synchronization (jitter) in collective communication tasks
and thus results in variable (degraded) overall parallel application
performance. This behavior is more observable in large-scale
applications using certain types of collective communication
primitives, such as MPI_Allreduce. Our efforts focused towards
reducing the overall effect of OS noise on our large-scale parallel
applications. We performed tests on the quad-core Jaguar, the Cray XT5
at the Oak Ridge National Laboratory Leadership Computing Facility
(OLCF). At the time of these tests, Jaguar was a 1.4 petaflop/second
supercomputer with 144,000 compute cores and 8 cores per node. The
technique we used was to aggregate and merge all OS noise sources onto
a single compute core for each node. The scientific application was
then run on the remaining seven cores in each node. Our results showed
that we were able to improve the MPI_Allreduce performance by two
orders of magnitude and to boost the Parallel Ocean Program (POP)
performance over 30% using this technique.


<p>
Contributors: Dave Dillow, Don Maxwell, Ross Miller, Sarp Oral, Galen
Shipman, and Feiyi Wang, Jeff Becklehimer (Cray), and Jeff Larkin (Cray)

<p>
Status: Completed

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')
-->

<!-- ========================================================== -->
<!--
%include('content_intro.inc')
    <h2 id="Process-Placement">Improving Application Performance by Avoiding Floating-Point Unit Contention on Titan</h2>

<p>Titan's compute nodes include an AMD Opteron&#8482; 6274 (Interlagos) CPU and a Nvidia Kepler&#8482; GPGPU. Applications may use the CPU, GPGPU, or both. The <a href="https://www.olcf.ornl.gov/kb_articles/xk7-cpu-description/" target="other">Interlagos CPU</a> has eight <i>bulldozer</i> modules. Each bulldozer has two integer units and one floating-point unit. Applications, which are floating-point intensive and which use the CPU for some or all computation, may see improved performance (i.e. reduced walltime) if they place one process per bulldozer. Typical application speedup is 1.5-2.0, with an average of 1.7.</p>

<p>This effort provides feedback to the user when their job submission tries to reduce floating-point unit contention, but the user fails to set the requisite flags to obtain the improved performance. We are also exploring the ability to provide alternative job submission options, which will set the appropriate flags for the user based on the type of job.</p>

    <p>Contributors: Scott Atchley and Sudharshan Vazhkudai

    <p>Status: On-going

  <p>&nbsp; <a class="right" href="#">Top</a>
%include('content_close.inc')

%include('html_close.inc')
-->
